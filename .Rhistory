{
install.packages("data.table")
}
if(!require("ggplot2"))
{
install.packages("ggplot2")
}
if(!require("caTools"))
{
install.packages("caTools")
}
# attach all functions provided by these packages
library(data.table)
library(ggplot2)
library(caTools)
# source NNetOneSplit function
source('NNetOneSplit.R')
#download spam data set to local directory, if it is not present
if(!file.exists("spam.data"))
{
download.file("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data", "spam.data")
}
# Read spam data set and convert to input matrix
spam.dt <- data.table::fread("spam.data")
N.obs <- nrow(spam.dt)
X.raw <- as.matrix(spam.dt[, -ncol(spam.dt), with=FALSE])
y.vec <- spam.dt[[ncol(spam.dt)]]
X.mat <- scale(X.raw)
max.epochs <- 10
step.size <- 0.05
n.hidden.units <- c(ncol(X.mat), 20, 1)
# Create vector with size = num observations in the whole data set
# elements are TRUE if observation is in train set, FALSE otherwise. Should be 80T-20F
# is.train contains rows which are in the train set. All other rows not in is.train are in the test set
is.train <- sample.int(n = nrow(X.mat), size = floor(.8*nrow(X.mat)), replace = F)
# create vector with size = num observations in the whole data set
# elements are TRUE if observation is in train set, FALSE otherwise. Should be 60T-40F
# is.subtrain contains rows which are in the train set. All other rows not in is.subtrain are in test set
is.subtrain <- sample.int(n = nrow(X.mat), size = floor(.6*nrow(X.mat)), replace = F)
# Call NNetOne with x.mat, y.vec, with is.subtrain, and large max.epochs
stuff <- NNetOneSplit(X.mat, y.vec, max.epochs, step.size, n.hidden.units, is.subtrain)
# Plot subtrain/validation loss as fxn of num epochs, draw point to emphasize min validation loss
loss.dt <- do.call(rbind, loss.values)
ggplot()+
geom_line(aes(
x=epoch, y=mean.loss, color=set),
data=loss.dt)
# best_epochs = epochs to minimize validation loss
best_epochs <- 0.5
# Call NNetONe with x.mat =, y.vec, is.subtrain = TRUE for all, max.epochs = best_epochs
stuff <- NNetOneSplit(X.mat, y.vec, is.subtrain=TRUE, best_epochs)
# Use learned V.mat, w.vec, to make predictions on test set
predictions <- ComputePredictions()
ComputePredictions <- function(X_train, y_train, X_new) class::knn(X_train, y_train, X_new)
n.folds <- 5
# install packages if it is not already
if(!require("data.table"))
{
install.packages("data.table")
}
if(!require("ggplot2"))
{
install.packages("ggplot2")
}
if(!require("caTools"))
{
install.packages("caTools")
}
# attach all functions provided by these packages
library(data.table)
library(ggplot2)
library(caTools)
# source NNetOneSplit function
source('NNetOneSplit.R')
#download spam data set to local directory, if it is not present
if(!file.exists("spam.data"))
{
download.file("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data", "spam.data")
}
# Read spam data set and convert to input matrix
spam.dt <- data.table::fread("spam.data")
N.obs <- nrow(spam.dt)
X.raw <- as.matrix(spam.dt[, -ncol(spam.dt), with=FALSE])
y.vec <- spam.dt[[ncol(spam.dt)]]
X.mat <- scale(X.raw)
max.epochs <- 10
step.size <- 0.05
n.hidden.units <- c(ncol(X.mat), 20, 1)
# Create vector with size = num observations in the whole data set
# elements are TRUE if observation is in train set, FALSE otherwise. Should be 80T-20F
# is.train contains rows which are in the train set. All other rows not in is.train are in the test set
is.train <- sample.int(n = nrow(X.mat), size = floor(.8*nrow(X.mat)), replace = F)
# create vector with size = num observations in the whole data set
# elements are TRUE if observation is in train set, FALSE otherwise. Should be 60T-40F
# is.subtrain contains rows which are in the train set. All other rows not in is.subtrain are in test set
is.subtrain <- sample.int(n = nrow(X.mat), size = floor(.6*nrow(X.mat)), replace = F)
# Call NNetOne with x.mat, y.vec, with is.subtrain, and large max.epochs
s
n.folds <- 5
set.seed(1)
fold.vec <- sample(rep(1:n.folds, l=length(y.vec)))
# lines 7-11 were copied from Hocking's R script. I believe these are false, and should be using is.subtrain
validation.fold <- 1
is.validation <- fold.vec == validation.fold
is.train <- !is.validation
table(is.train)
# is.subtrain should be defining y.train and X.train
yt.train <- y.ec[which(is.subtrain)]
X.train <- X.matec[which(is.subtrain)]
#loss.values <- list()
set.seed(1)
weight.mat.list <- list()
for(layer.i in 1:(length(n.hidden.units)-1)){
mat.nrow <- n.hidden.units[[layer.i+1]]
mat.ncol <- n.hidden.units[[layer.i]]
weight.mat.list[[layer.i]] <- matrix(
rnorm(mat.nrow*mat.ncol), mat.nrow, mat.ncol)
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
obs.i <- is.train[[layer.i]]
v <- X.train[obs.i,]
w <- yt.train[obs.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
n.folds <- 5
set.seed(1)
fold.vec <- sample(rep(1:n.folds, l=length(y.vec)))
# lines 7-11 were copied from Hocking's R script. I believe these are false, and should be using is.subtrain
validation.fold <- 1
is.validation <- fold.vec == validation.fold
is.train <- !is.validation
table(is.train)
# is.subtrain should be defining y.train and X.train
yt.train <- y.ec[which(is.subtrain)]
X.train <- X.matec[which(is.subtrain)]
#loss.values <- list()
set.seed(1)
weight.mat.list <- list()
for(layer.i in 1:(length(n.hidden.units)-1)){
mat.nrow <- n.hidden.units[[layer.i+1]]
mat.ncol <- n.hidden.units[[layer.i]]
weight.mat.list[[layer.i]] <- matrix(
rnorm(mat.nrow*mat.ncol), mat.nrow, mat.ncol)
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
obs.i <- is.train[[layer.i]]
v <- X.train[obs.i,]
w <- yt.train[obs.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
n.folds <- 5
set.seed(1)
fold.vec <- sample(rep(1:n.folds, l=length(y.vec)))
# lines 7-11 were copied from Hocking's R script. I believe these are false, and should be using is.subtrain
validation.fold <- 1
is.validation <- fold.vec == validation.fold
is.train <- !is.validation
table(is.train)
# is.subtrain should be defining y.train and X.train
yt.train <- y.vec[which(is.subtrain)]
X.train <- X.matec[which(is.subtrain),]
n.folds <- 5
set.seed(1)
fold.vec <- sample(rep(1:n.folds, l=length(y.vec)))
# lines 7-11 were copied from Hocking's R script. I believe these are false, and should be using is.subtrain
validation.fold <- 1
is.validation <- fold.vec == validation.fold
is.train <- !is.validation
table(is.train)
# is.subtrain should be defining y.train and X.train
yt.train <- y.vec[which(is.subtrain)]
X.train <- X.mat[which(is.subtrain)]
# is.subtrain should be defining y.train and X.train
yt.train <- y.vec[is.subtrain]
X.train <- X.mat[is.subtrain]
n.folds <- 5
set.seed(1)
fold.vec <- sample(rep(1:n.folds, l=length(y.vec)))
# lines 7-11 were copied from Hocking's R script. I believe these are false, and should be using is.subtrain
validation.fold <- 1
is.validation <- fold.vec == validation.fold
is.train <- !is.validation
table(is.train)
# is.subtrain should be defining y.train and X.train
yt.train <- y.vec[is.subtrain]
X.train <- X.mat[is.subtrain]
set.seed(1)
weight.mat.list <- list()
for(layer.i in 1:(length(n.hidden.units)-1)){
mat.nrow <- n.hidden.units[[layer.i+1]]
mat.ncol <- n.hidden.units[[layer.i]]
weight.mat.list[[layer.i]] <- matrix(
rnorm(mat.nrow*mat.ncol), mat.nrow, mat.ncol)
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
obs.i <- is.train[[layer.i]]
v <- X.train[obs.i,]
w <- yt.train[obs.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
obs.i <- is.subtrain[[layer.i]]
v <- X.train[obs.i,]
w <- yt.train[obs.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
layer.i <- is.subtrain[[layer.i]]
v <- X.train[obs.i,]
w <- yt.train[obs.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
layer.i <- is.subtrain[[layer.i]]
v <- X.train[layeri,]
w <- yt.train[layer.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
layer.i <- is.subtrain[[layer.i]]
v <- X.train[layer.i,]
w <- yt.train[layer.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
#layer.i <- is.subtrain[[layer.i]]
v <- X.train[layer.i,]
w <- yt.train[layer.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
obs.i <- is.subtrain[[layer.i]]
v <- X.train[obs.i,]
w <- yt.train[obs.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
##create a for loop over epochs
for(layer.i in 1:(length(n.hidden.units)-1))
{
obs.i <- is.subtrain[[layer.i]]
v <- X.train[obs.i,]
w <- yt.train[obs.i]
w.list <- 1/(1+exp(-w*v))
grad.list <- list()
for(layer.i in length(weight.mat.list):1)
{
loss.values <- if(layer.i==length(weight.mat.list))
{
w / (1+exp(w*w.list[[length(w.list)]]))
}
else
{
# grad.w <- t(weight.mat.list[[layer.i+1]]) %*% loss.values
grad.w <- t(weight.mat.list[[layer.i+1]]) * loss.values
w.vec <- w.list[[layer.i+1]]
grad.w * w.vec * (1-w.vec)
}
#grad.list[[layer.i]] <- loss.values %*% t(w.list[[layer.i]])
grad.list[[lagetyer.i]] <- loss.values %*% t(grad.w)
}
v.mat <- weight.mat.list
print(dim(grad.list[[layer.i]]))
print(dim(v.mat[[layer.i]] - step.size))
for(epoch in 1:max.epochs)
{
# v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
v.mat[[layer.i]] <- v.mat[[layer.i]] - step.size * grad.list[[layer.i]]
}
}
